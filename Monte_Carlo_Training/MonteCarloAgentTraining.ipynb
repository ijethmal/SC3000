{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Monte Carlo Agent for Cartpole Problem"
      ],
      "metadata": {
        "id": "ZauhjPSfX7pI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import RecordVideo\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import NamedTuple\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")"
      ],
      "metadata": {
        "id": "j6KpgCLGYWmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")"
      ],
      "metadata": {
        "id": "Go12dH4qbwBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.observation_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyqHr9I5cdkX",
        "outputId": "18029438-4fbf-4655-ba5d-e2d5a398fbe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Observation(NamedTuple):\n",
        "    cart_pos: int\n",
        "    cart_vel: int\n",
        "    pole_angle: int\n",
        "    pole_vel: int\n",
        "\n",
        "def discretize_state(observation):\n",
        "  #\n",
        "    cart_pos_bins = np.linspace(-4.8, 4.8, 10)\n",
        "    cart_vel_bins = np.linspace(-5, 5, 10)\n",
        "    pole_angle_bins = np.linspace(-0.418, 0.418, 10)\n",
        "    pole_vel_bins = np.linspace(-5, 5, 10)\n",
        "\n",
        "    return Observation(\n",
        "        np.digitize(observation[0], cart_pos_bins),\n",
        "        np.digitize(observation[1], cart_vel_bins),\n",
        "        np.digitize(observation[2], pole_angle_bins),\n",
        "        np.digitize(observation[3], pole_vel_bins)\n",
        "    )"
      ],
      "metadata": {
        "id": "7r7ghR9zX7YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Development of an RL agent"
      ],
      "metadata": {
        "id": "2oIzK9SzhlWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Reinforcement Learning, we have an agent that interacts with the environment by performing an action (A) at each time step t. The environment then responds to the action with a new state (S) and reward (R). The agent takes the new state and reward into consideration when performing yet another action. The agent‚Äôs goal is to perform actions that will maximize the total reward it receives over the long run.\n",
        "\n",
        "**Action Selection Strategy: Epsilon-Greedy**\n",
        "\n",
        "How each action is chosen is defined within choose_action. The value of self-epsilon determines the amount of exploration and exploitation- so a self.epsilon = 0.2 would mean 20% exploration. If np.random.uniform(0,1) returns a value less than self.epsilon, the agent will explore - which means choose a random action.\n",
        "\n",
        "If np.random.uniform(0,1) returns a value higher than self.epsilon, the agent will exploit - which means choose the action with the highest estimated Q-value. (explained below)\n",
        "\n",
        "A high epsilon encourages more exploration, while a low epsilon favors exploiting the current best actions. The value of epsilon needs to be tuned for the specific problem, so we try different values to find the value that creates the best agent.\n",
        "\n",
        "**Q-Value Updating**\n",
        "\n",
        "*Return* refers to the function of the discounted reward sequence with discount rate ùõæ (0 ‚â§ ùõæ ‚â§ 1). Return G at time step t is calculated as follows:\n",
        "```\n",
        "G_t = R_{t+1} + Œ≥ * R_{t+2} + Œ≥^2 * R_{t+3} + ...\n",
        "```\n",
        "A higher gamma value gives more weight to future rewards, and the limit is 1, at which point future rewards are just as important as immediate rewards. If gamma is 0, the agent only considers immediate rewards. In this code, we tried different values of gamma to find the optimal discount rate for this problem.\n",
        "\n",
        "*Policy* is a mapping from states to the probabilities of selecting each possible action. The policy is what we want to learn and optimize so that it maximizes the expected return over the long run.\n",
        "\n",
        "*Value Function* is a function that estimates the expected return starting from a given state (or state-action pair) and following a policy. This agent uses action-value function, or Q-function, which is a type of value function that estimates the value of taking a specific action (A) in a particular state (S).\n",
        "\n",
        "Our agent interacts with the environment, runs full episodes and after each episode, runs update_Q to update its Q-value estimates. The update rule in update_Q updates the Q-value of a state-action pair using the following formula:\n",
        "```\n",
        "`Q(s, a) = Q(s, a) + Œ± * [R + Œ≥ * max_a' Q(s', a') - Q(s, a)]`\n",
        "```\n",
        "The formula updates Q-value of a state-action pair (Q(s,a)) based on the difference between the actual return (G) observed during an episode and the current estimated Q-value. The learning rate (alpha) controls the magnitude of this adjustment. This is **Monte Carlo Q-Evaluation**.\n",
        "\n",
        "The hyperparameters, thus, are as follows:\n",
        "\n",
        "1. Alpha\n",
        "2. Gamma\n",
        "3. Epsilon\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "072v-GtAta76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MonteCarloAgent: # random default values for hyperparameters\n",
        "    def __init__(self, action_space_size, load_from_file=True, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
        "      self.action_space_size = action_space_size\n",
        "      self.alpha = alpha  # learning rate\n",
        "      self.gamma = gamma  # discount factor\n",
        "      self.epsilon = epsilon  # exploration %\n",
        "      self.epsilon_min = epsilon_min\n",
        "      self.epsilon_decay = epsilon_decay\n",
        "\n",
        "      # use defaultdict to dynamically store Q-values\n",
        "      self.Q = defaultdict(lambda: 10)  # high Q-values for optimism\n",
        "      self.returns = defaultdict(list)\n",
        "      self.N = defaultdict(int)\n",
        "      self.filename = \"mc_q_table.pkl\"\n",
        "\n",
        "      # load Q-table if available\n",
        "      if load_from_file:\n",
        "        self.load_q_table()\n",
        "\n",
        "    def save_q_table(self):\n",
        "      with open(self.filename, \"wb\") as file:\n",
        "        pickle.dump((dict(self.Q), dict(self.returns), dict(self.N)), file)\n",
        "\n",
        "    def load_q_table(self):\n",
        "      try:\n",
        "        with open(self.filename, \"rb\") as file:\n",
        "          self.Q, self.returns, self.N = pickle.load(file)\n",
        "          self.Q = defaultdict(int, self.Q)\n",
        "          self.returns = defaultdict(list, self.returns)\n",
        "          self.N = defaultdict(int, self.N)\n",
        "      except (EOFError, FileNotFoundError):\n",
        "        print(\"No saved Q-table found. starting fresh.\")\n",
        "\n",
        "\n",
        "    # decay epsilon over time to encourage exploitation over time\n",
        "    def decay_epsilon(self):\n",
        "      self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def reset_q_table(self):\n",
        "      self.Q = defaultdict(lambda: 10)\n",
        "      self.returns = defaultdict(list)\n",
        "      self.N = defaultdict(int)\n",
        "      self.save_q_table()\n",
        "\n",
        "    def choose_action(self, state):\n",
        "      # epsilon-greedy action selection\n",
        "      if np.random.uniform(0, 1) < self.epsilon:\n",
        "        return np.random.choice(self.action_space_size) # explore\n",
        "      else:\n",
        "        return np.argmax([self.Q[(state, a)] for a in range(self.action_space_size)]) # exploit\n",
        "\n",
        "    # update_Q will be called at the end of each episode. it takes the entire episode's experience as input\n",
        "    # and calculates the return G for each state-action pair throughout the episode and accumulating the\n",
        "    # rewards. Q-value for each state-action pair is updated using the calculated return G\n",
        "    def update_Q(self, episode):\n",
        "      G = 0\n",
        "      visited_state_action_pairs = set()\n",
        "\n",
        "      for t in reversed(range(len(episode))):\n",
        "        state, action, reward = episode[t]\n",
        "        G = self.gamma * G + reward\n",
        "\n",
        "        # first-visit Monte Carlo - compared to every-visit, often preferred due to its faster convergence\n",
        "        if (state, action) not in visited_state_action_pairs:\n",
        "          visited_state_action_pairs.add((state, action))\n",
        "          self.returns[(state, action)].append(G)\n",
        "          self.N[state, action] += 1\n",
        "\n",
        "          # updating Q-value using the learning rate (alpha)\n",
        "          self.Q[state, action] += self.alpha * (G - self.Q[state, action])\n",
        "\n",
        "      self.save_q_table()  # save progress after every episode"
      ],
      "metadata": {
        "id": "E6iJuAttQ_QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we train the agent, using a wide range of values for each hyperparameter (alpha, gamma, epsilon) from 0 to 1, to find if higher or lower values are optimal for each hyperparameter, in the context of this problem.\n",
        "\n",
        "For each combination of hyperparameters we run 3 trials of 500 episodes and the results dictionary stores the average reward across trials.\n",
        "\n",
        "We will then use the hyperparameter combination with the highest average rewards to train a best agent."
      ],
      "metadata": {
        "id": "GImmqANl7C_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphas = [0.1, 0.25, 0.5, 0.75] # lower to higher learning rate\n",
        "gammas = [0.5, 0.7, 0.9, 0.99]  # lower to higher discount rate\n",
        "epsilons = [0.2, 0.4, 0.6]  # less to more exploration\n",
        "\n",
        "num_trials = 3  # number of trials per combination\n",
        "num_episodes = 500\n",
        "\n",
        "results = {}\n",
        "\n",
        "for alpha in alphas:\n",
        "  for gamma in gammas:\n",
        "    for epsilon in epsilons:\n",
        "      trial_rewards = []\n",
        "      for trial in range(num_trials):\n",
        "        agent = MonteCarloAgent(env.action_space.n, alpha=alpha, gamma=gamma, epsilon=epsilon)\n",
        "        agent.reset_q_table()\n",
        "        for episode in range(num_episodes):\n",
        "          observation = env.reset()\n",
        "          episode = []\n",
        "          episode_rewards = []  # rewards for each episode within a trial\n",
        "          cumulative_reward = 0\n",
        "          done = False\n",
        "\n",
        "          while not done:\n",
        "            state = discretize_state(observation)\n",
        "            action = agent.choose_action(state)\n",
        "            observation, reward, done, info = env.step(action)\n",
        "\n",
        "            cart_pos, cart_vel, pole_angle, pole_vel = observation\n",
        "\n",
        "            # termination check\n",
        "            if abs(pole_angle) > 0.2094 or abs(cart_pos) > 2.4:\n",
        "              done = True\n",
        "\n",
        "            episode.append((state, action, reward))\n",
        "            cumulative_reward += reward\n",
        "\n",
        "          agent.update_Q(episode)\n",
        "          agent.decay_epsilon()\n",
        "          episode_rewards.append(cumulative_reward)  # rewards for episode\n",
        "\n",
        "        trial_rewards.append(np.mean(episode_rewards))  # average reward for trial\n",
        "        print(f\"Trial {trial + 1} for alpha={alpha}, gamma={gamma}, epsilon={epsilon} complete.\")\n",
        "\n",
        "      results[(alpha, gamma, epsilon)] = np.mean(trial_rewards)  # average reward across trials\n",
        "      print(f\"Trials for alpha={alpha}, gamma={gamma}, epsilon={epsilon} complete. Average reward: {results[(alpha, gamma, epsilon)]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdlW6B0a7418",
        "outputId": "d344f2dc-e81e-423c-d08a-d00ea19b31b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No saved Q-table found. starting fresh.\n",
            "Trial 1 for alpha=0.1, gamma=0.5, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.5, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.5, epsilon=0.2 complete.\n",
            "Trials for alpha=0.1, gamma=0.5, epsilon=0.2 complete. Average reward: 88.0\n",
            "Trial 1 for alpha=0.1, gamma=0.5, epsilon=0.4 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.5, epsilon=0.4 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.5, epsilon=0.4 complete.\n",
            "Trials for alpha=0.1, gamma=0.5, epsilon=0.4 complete. Average reward: 106.0\n",
            "Trial 1 for alpha=0.1, gamma=0.5, epsilon=0.6 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.5, epsilon=0.6 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.5, epsilon=0.6 complete.\n",
            "Trials for alpha=0.1, gamma=0.5, epsilon=0.6 complete. Average reward: 97.33333333333333\n",
            "Trial 1 for alpha=0.1, gamma=0.7, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.7, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.7, epsilon=0.2 complete.\n",
            "Trials for alpha=0.1, gamma=0.7, epsilon=0.2 complete. Average reward: 105.66666666666667\n",
            "Trial 1 for alpha=0.1, gamma=0.7, epsilon=0.4 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.7, epsilon=0.4 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.7, epsilon=0.4 complete.\n",
            "Trials for alpha=0.1, gamma=0.7, epsilon=0.4 complete. Average reward: 123.66666666666667\n",
            "Trial 1 for alpha=0.1, gamma=0.7, epsilon=0.6 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.7, epsilon=0.6 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.7, epsilon=0.6 complete.\n",
            "Trials for alpha=0.1, gamma=0.7, epsilon=0.6 complete. Average reward: 149.0\n",
            "Trial 1 for alpha=0.1, gamma=0.9, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.9, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.9, epsilon=0.2 complete.\n",
            "Trials for alpha=0.1, gamma=0.9, epsilon=0.2 complete. Average reward: 151.33333333333334\n",
            "Trial 1 for alpha=0.1, gamma=0.9, epsilon=0.4 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.9, epsilon=0.4 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.9, epsilon=0.4 complete.\n",
            "Trials for alpha=0.1, gamma=0.9, epsilon=0.4 complete. Average reward: 151.66666666666666\n",
            "Trial 1 for alpha=0.1, gamma=0.9, epsilon=0.6 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.9, epsilon=0.6 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.9, epsilon=0.6 complete.\n",
            "Trials for alpha=0.1, gamma=0.9, epsilon=0.6 complete. Average reward: 170.66666666666666\n",
            "Trial 1 for alpha=0.1, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.99, epsilon=0.2 complete.\n",
            "Trials for alpha=0.1, gamma=0.99, epsilon=0.2 complete. Average reward: 353.6666666666667\n",
            "Trial 1 for alpha=0.1, gamma=0.99, epsilon=0.4 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.99, epsilon=0.4 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.99, epsilon=0.4 complete.\n",
            "Trials for alpha=0.1, gamma=0.99, epsilon=0.4 complete. Average reward: 127.0\n",
            "Trial 1 for alpha=0.1, gamma=0.99, epsilon=0.6 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.99, epsilon=0.6 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.99, epsilon=0.6 complete.\n",
            "Trials for alpha=0.1, gamma=0.99, epsilon=0.6 complete. Average reward: 86.66666666666667\n",
            "Trial 1 for alpha=0.25, gamma=0.5, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.25, gamma=0.5, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.25, gamma=0.5, epsilon=0.2 complete.\n",
            "Trials for alpha=0.25, gamma=0.5, epsilon=0.2 complete. Average reward: 193.33333333333334\n",
            "Trial 1 for alpha=0.25, gamma=0.5, epsilon=0.4 complete.\n",
            "Trial 2 for alpha=0.25, gamma=0.5, epsilon=0.4 complete.\n",
            "Trial 3 for alpha=0.25, gamma=0.5, epsilon=0.4 complete.\n",
            "Trials for alpha=0.25, gamma=0.5, epsilon=0.4 complete. Average reward: 103.33333333333333\n",
            "Trial 1 for alpha=0.25, gamma=0.5, epsilon=0.6 complete.\n",
            "Trial 2 for alpha=0.25, gamma=0.5, epsilon=0.6 complete.\n",
            "Trial 3 for alpha=0.25, gamma=0.5, epsilon=0.6 complete.\n",
            "Trials for alpha=0.25, gamma=0.5, epsilon=0.6 complete. Average reward: 97.33333333333333\n",
            "Trial 1 for alpha=0.25, gamma=0.7, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.25, gamma=0.7, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.25, gamma=0.7, epsilon=0.2 complete.\n",
            "Trials for alpha=0.25, gamma=0.7, epsilon=0.2 complete. Average reward: 119.66666666666667\n",
            "Trial 1 for alpha=0.25, gamma=0.7, epsilon=0.4 complete.\n",
            "Trial 2 for alpha=0.25, gamma=0.7, epsilon=0.4 complete.\n",
            "Trial 3 for alpha=0.25, gamma=0.7, epsilon=0.4 complete.\n",
            "Trials for alpha=0.25, gamma=0.7, epsilon=0.4 complete. Average reward: 205.0\n",
            "Trial 1 for alpha=0.25, gamma=0.7, epsilon=0.6 complete.\n",
            "Trial 2 for alpha=0.25, gamma=0.7, epsilon=0.6 complete.\n",
            "Trial 3 for alpha=0.25, gamma=0.7, epsilon=0.6 complete.\n",
            "Trials for alpha=0.25, gamma=0.7, epsilon=0.6 complete. Average reward: 223.0\n",
            "Trial 1 for alpha=0.25, gamma=0.9, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.25, gamma=0.9, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.25, gamma=0.9, epsilon=0.2 complete.\n",
            "Trials for alpha=0.25, gamma=0.9, epsilon=0.2 complete. Average reward: 189.33333333333334\n",
            "Trial 1 for alpha=0.25, gamma=0.9, epsilon=0.4 complete.\n",
            "Trial 2 for alpha=0.25, gamma=0.9, epsilon=0.4 complete.\n",
            "Trial 3 for alpha=0.25, gamma=0.9, epsilon=0.4 complete.\n",
            "Trials for alpha=0.25, gamma=0.9, epsilon=0.4 complete. Average reward: 176.0\n",
            "Trial 1 for alpha=0.25, gamma=0.9, epsilon=0.6 complete.\n",
            "Trial 2 for alpha=0.25, gamma=0.9, epsilon=0.6 complete.\n",
            "Trial 3 for alpha=0.25, gamma=0.9, epsilon=0.6 complete.\n",
            "Trials for alpha=0.25, gamma=0.9, epsilon=0.6 complete. Average reward: 162.0\n",
            "Trial 1 for alpha=0.25, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.25, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.25, gamma=0.99, epsilon=0.2 complete.\n",
            "Trials for alpha=0.25, gamma=0.99, epsilon=0.2 complete. Average reward: 207.0\n",
            "Trial 1 for alpha=0.25, gamma=0.99, epsilon=0.4 complete.\n",
            "Trial 2 for alpha=0.25, gamma=0.99, epsilon=0.4 complete.\n",
            "Trial 3 for alpha=0.25, gamma=0.99, epsilon=0.4 complete.\n",
            "Trials for alpha=0.25, gamma=0.99, epsilon=0.4 complete. Average reward: 93.66666666666667\n",
            "Trial 1 for alpha=0.25, gamma=0.99, epsilon=0.6 complete.\n",
            "Trial 2 for alpha=0.25, gamma=0.99, epsilon=0.6 complete.\n",
            "Trial 3 for alpha=0.25, gamma=0.99, epsilon=0.6 complete.\n",
            "Trials for alpha=0.25, gamma=0.99, epsilon=0.6 complete. Average reward: 170.0\n",
            "Trial 1 for alpha=0.5, gamma=0.5, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.5, gamma=0.5, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.5, gamma=0.5, epsilon=0.2 complete.\n",
            "Trials for alpha=0.5, gamma=0.5, epsilon=0.2 complete. Average reward: 113.0\n",
            "Trial 1 for alpha=0.5, gamma=0.5, epsilon=0.4 complete.\n",
            "Trial 2 for alpha=0.5, gamma=0.5, epsilon=0.4 complete.\n",
            "Trial 3 for alpha=0.5, gamma=0.5, epsilon=0.4 complete.\n",
            "Trials for alpha=0.5, gamma=0.5, epsilon=0.4 complete. Average reward: 117.66666666666667\n",
            "Trial 1 for alpha=0.5, gamma=0.5, epsilon=0.6 complete.\n",
            "Trial 2 for alpha=0.5, gamma=0.5, epsilon=0.6 complete.\n",
            "Trial 3 for alpha=0.5, gamma=0.5, epsilon=0.6 complete.\n",
            "Trials for alpha=0.5, gamma=0.5, epsilon=0.6 complete. Average reward: 161.0\n",
            "Trial 1 for alpha=0.5, gamma=0.7, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.5, gamma=0.7, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.5, gamma=0.7, epsilon=0.2 complete.\n",
            "Trials for alpha=0.5, gamma=0.7, epsilon=0.2 complete. Average reward: 159.0\n",
            "Trial 1 for alpha=0.5, gamma=0.7, epsilon=0.4 complete.\n",
            "Trial 2 for alpha=0.5, gamma=0.7, epsilon=0.4 complete.\n",
            "Trial 3 for alpha=0.5, gamma=0.7, epsilon=0.4 complete.\n",
            "Trials for alpha=0.5, gamma=0.7, epsilon=0.4 complete. Average reward: 164.33333333333334\n",
            "Trial 1 for alpha=0.5, gamma=0.7, epsilon=0.6 complete.\n",
            "Trial 2 for alpha=0.5, gamma=0.7, epsilon=0.6 complete.\n",
            "Trial 3 for alpha=0.5, gamma=0.7, epsilon=0.6 complete.\n",
            "Trials for alpha=0.5, gamma=0.7, epsilon=0.6 complete. Average reward: 172.0\n",
            "Trial 1 for alpha=0.5, gamma=0.9, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.5, gamma=0.9, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.5, gamma=0.9, epsilon=0.2 complete.\n",
            "Trials for alpha=0.5, gamma=0.9, epsilon=0.2 complete. Average reward: 101.33333333333333\n",
            "Trial 1 for alpha=0.5, gamma=0.9, epsilon=0.4 complete.\n",
            "Trial 2 for alpha=0.5, gamma=0.9, epsilon=0.4 complete.\n",
            "Trial 3 for alpha=0.5, gamma=0.9, epsilon=0.4 complete.\n",
            "Trials for alpha=0.5, gamma=0.9, epsilon=0.4 complete. Average reward: 95.66666666666667\n",
            "Trial 1 for alpha=0.5, gamma=0.9, epsilon=0.6 complete.\n",
            "Trial 2 for alpha=0.5, gamma=0.9, epsilon=0.6 complete.\n",
            "Trial 3 for alpha=0.5, gamma=0.9, epsilon=0.6 complete.\n",
            "Trials for alpha=0.5, gamma=0.9, epsilon=0.6 complete. Average reward: 186.0\n",
            "Trial 1 for alpha=0.5, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.5, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.5, gamma=0.99, epsilon=0.2 complete.\n",
            "Trials for alpha=0.5, gamma=0.99, epsilon=0.2 complete. Average reward: 123.66666666666667\n",
            "Trial 1 for alpha=0.5, gamma=0.99, epsilon=0.4 complete.\n",
            "Trial 2 for alpha=0.5, gamma=0.99, epsilon=0.4 complete.\n",
            "Trial 3 for alpha=0.5, gamma=0.99, epsilon=0.4 complete.\n",
            "Trials for alpha=0.5, gamma=0.99, epsilon=0.4 complete. Average reward: 122.66666666666667\n",
            "Trial 1 for alpha=0.5, gamma=0.99, epsilon=0.6 complete.\n",
            "Trial 2 for alpha=0.5, gamma=0.99, epsilon=0.6 complete.\n",
            "Trial 3 for alpha=0.5, gamma=0.99, epsilon=0.6 complete.\n",
            "Trials for alpha=0.5, gamma=0.99, epsilon=0.6 complete. Average reward: 37.666666666666664\n",
            "Trial 1 for alpha=0.75, gamma=0.5, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.75, gamma=0.5, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.75, gamma=0.5, epsilon=0.2 complete.\n",
            "Trials for alpha=0.75, gamma=0.5, epsilon=0.2 complete. Average reward: 137.0\n",
            "Trial 1 for alpha=0.75, gamma=0.5, epsilon=0.4 complete.\n",
            "Trial 2 for alpha=0.75, gamma=0.5, epsilon=0.4 complete.\n",
            "Trial 3 for alpha=0.75, gamma=0.5, epsilon=0.4 complete.\n",
            "Trials for alpha=0.75, gamma=0.5, epsilon=0.4 complete. Average reward: 85.0\n",
            "Trial 1 for alpha=0.75, gamma=0.5, epsilon=0.6 complete.\n",
            "Trial 2 for alpha=0.75, gamma=0.5, epsilon=0.6 complete.\n",
            "Trial 3 for alpha=0.75, gamma=0.5, epsilon=0.6 complete.\n",
            "Trials for alpha=0.75, gamma=0.5, epsilon=0.6 complete. Average reward: 66.66666666666667\n",
            "Trial 1 for alpha=0.75, gamma=0.7, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.75, gamma=0.7, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.75, gamma=0.7, epsilon=0.2 complete.\n",
            "Trials for alpha=0.75, gamma=0.7, epsilon=0.2 complete. Average reward: 114.0\n",
            "Trial 1 for alpha=0.75, gamma=0.7, epsilon=0.4 complete.\n",
            "Trial 2 for alpha=0.75, gamma=0.7, epsilon=0.4 complete.\n",
            "Trial 3 for alpha=0.75, gamma=0.7, epsilon=0.4 complete.\n",
            "Trials for alpha=0.75, gamma=0.7, epsilon=0.4 complete. Average reward: 132.0\n",
            "Trial 1 for alpha=0.75, gamma=0.7, epsilon=0.6 complete.\n",
            "Trial 2 for alpha=0.75, gamma=0.7, epsilon=0.6 complete.\n",
            "Trial 3 for alpha=0.75, gamma=0.7, epsilon=0.6 complete.\n",
            "Trials for alpha=0.75, gamma=0.7, epsilon=0.6 complete. Average reward: 145.0\n",
            "Trial 1 for alpha=0.75, gamma=0.9, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.75, gamma=0.9, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.75, gamma=0.9, epsilon=0.2 complete.\n",
            "Trials for alpha=0.75, gamma=0.9, epsilon=0.2 complete. Average reward: 103.33333333333333\n",
            "Trial 1 for alpha=0.75, gamma=0.9, epsilon=0.4 complete.\n",
            "Trial 2 for alpha=0.75, gamma=0.9, epsilon=0.4 complete.\n",
            "Trial 3 for alpha=0.75, gamma=0.9, epsilon=0.4 complete.\n",
            "Trials for alpha=0.75, gamma=0.9, epsilon=0.4 complete. Average reward: 195.66666666666666\n",
            "Trial 1 for alpha=0.75, gamma=0.9, epsilon=0.6 complete.\n",
            "Trial 2 for alpha=0.75, gamma=0.9, epsilon=0.6 complete.\n",
            "Trial 3 for alpha=0.75, gamma=0.9, epsilon=0.6 complete.\n",
            "Trials for alpha=0.75, gamma=0.9, epsilon=0.6 complete. Average reward: 115.33333333333333\n",
            "Trial 1 for alpha=0.75, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.75, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.75, gamma=0.99, epsilon=0.2 complete.\n",
            "Trials for alpha=0.75, gamma=0.99, epsilon=0.2 complete. Average reward: 179.0\n",
            "Trial 1 for alpha=0.75, gamma=0.99, epsilon=0.4 complete.\n",
            "Trial 2 for alpha=0.75, gamma=0.99, epsilon=0.4 complete.\n",
            "Trial 3 for alpha=0.75, gamma=0.99, epsilon=0.4 complete.\n",
            "Trials for alpha=0.75, gamma=0.99, epsilon=0.4 complete. Average reward: 49.666666666666664\n",
            "Trial 1 for alpha=0.75, gamma=0.99, epsilon=0.6 complete.\n",
            "Trial 2 for alpha=0.75, gamma=0.99, epsilon=0.6 complete.\n",
            "Trial 3 for alpha=0.75, gamma=0.99, epsilon=0.6 complete.\n",
            "Trials for alpha=0.75, gamma=0.99, epsilon=0.6 complete. Average reward: 52.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = max(results, key=results.get)\n",
        "best_reward = results[best_params]\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Best Average Reward: {best_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1rzQj9ZAHpZ",
        "outputId": "17a3e1d2-deb7-4afc-d19b-958c1cc83009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: (0.1, 0.99, 0.2)\n",
            "Best Average Reward: 353.6666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know now that the best learning rate is on the lower end of 0 to 1, the best discount rate is on the higher end of 0 to 1 and the best epsilon value is on the lower end of 0 to 1.\n",
        "\n",
        "Let's run the hyperparameter tuning again, but with more values around the 'best range' specified for each value."
      ],
      "metadata": {
        "id": "JM3ErfYc7mPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphas = [0.01, 0.05, 0.1]\n",
        "gammas = [0.99, 0.995, 0.999]\n",
        "epsilons = [0.15, 0.20, 0.25]\n",
        "\n",
        "num_trials = 5  # more trials - outliers don't change the average too much\n",
        "num_episodes = 500\n",
        "\n",
        "results = {}\n",
        "\n",
        "for alpha in alphas:\n",
        "  for gamma in gammas:\n",
        "    for epsilon in epsilons:\n",
        "      trial_rewards = []\n",
        "      for trial in range(num_trials):\n",
        "        agent = MonteCarloAgent(env.action_space.n, alpha=alpha, gamma=gamma, epsilon=epsilon)\n",
        "        agent.reset_q_table()\n",
        "        for episode in range(num_episodes):\n",
        "          observation = env.reset()\n",
        "          episode = []\n",
        "          episode_rewards = []  # rewards for each episode within a trial\n",
        "          cumulative_reward = 0\n",
        "          done = False\n",
        "\n",
        "          while not done:\n",
        "            state = discretize_state(observation)\n",
        "            action = agent.choose_action(state)\n",
        "            observation, reward, done, info = env.step(action)\n",
        "\n",
        "            cart_pos, cart_vel, pole_angle, pole_vel = observation\n",
        "\n",
        "            # termination check\n",
        "            if abs(pole_angle) > 0.2094 or abs(cart_pos) > 2.4:\n",
        "              done = True\n",
        "\n",
        "            episode.append((state, action, reward))\n",
        "            cumulative_reward += reward\n",
        "\n",
        "          agent.update_Q(episode)\n",
        "          agent.decay_epsilon()\n",
        "          episode_rewards.append(cumulative_reward)  # rewards for episode\n",
        "\n",
        "        trial_rewards.append(np.mean(episode_rewards))  # average reward for trial\n",
        "        print(f\"Trial {trial + 1} for alpha={alpha}, gamma={gamma}, epsilon={epsilon} complete.\")\n",
        "\n",
        "      results[(alpha, gamma, epsilon)] = np.mean(trial_rewards)  # average reward across trials\n",
        "      print(f\"Trials for alpha={alpha}, gamma={gamma}, epsilon={epsilon} complete. Average reward: {results[(alpha, gamma, epsilon)]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYKBQDR67Pm9",
        "outputId": "291f5a94-5cf3-4484-eded-96cccbc81b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1 for alpha=0.01, gamma=0.99, epsilon=0.15 complete.\n",
            "Trial 2 for alpha=0.01, gamma=0.99, epsilon=0.15 complete.\n",
            "Trial 3 for alpha=0.01, gamma=0.99, epsilon=0.15 complete.\n",
            "Trial 4 for alpha=0.01, gamma=0.99, epsilon=0.15 complete.\n",
            "Trial 5 for alpha=0.01, gamma=0.99, epsilon=0.15 complete.\n",
            "Trials for alpha=0.01, gamma=0.99, epsilon=0.15 complete. Average reward: 111.4\n",
            "Trial 1 for alpha=0.01, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.01, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.01, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 4 for alpha=0.01, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 5 for alpha=0.01, gamma=0.99, epsilon=0.2 complete.\n",
            "Trials for alpha=0.01, gamma=0.99, epsilon=0.2 complete. Average reward: 185.8\n",
            "Trial 1 for alpha=0.01, gamma=0.99, epsilon=0.25 complete.\n",
            "Trial 2 for alpha=0.01, gamma=0.99, epsilon=0.25 complete.\n",
            "Trial 3 for alpha=0.01, gamma=0.99, epsilon=0.25 complete.\n",
            "Trial 4 for alpha=0.01, gamma=0.99, epsilon=0.25 complete.\n",
            "Trial 5 for alpha=0.01, gamma=0.99, epsilon=0.25 complete.\n",
            "Trials for alpha=0.01, gamma=0.99, epsilon=0.25 complete. Average reward: 140.2\n",
            "Trial 1 for alpha=0.01, gamma=0.995, epsilon=0.15 complete.\n",
            "Trial 2 for alpha=0.01, gamma=0.995, epsilon=0.15 complete.\n",
            "Trial 3 for alpha=0.01, gamma=0.995, epsilon=0.15 complete.\n",
            "Trial 4 for alpha=0.01, gamma=0.995, epsilon=0.15 complete.\n",
            "Trial 5 for alpha=0.01, gamma=0.995, epsilon=0.15 complete.\n",
            "Trials for alpha=0.01, gamma=0.995, epsilon=0.15 complete. Average reward: 164.2\n",
            "Trial 1 for alpha=0.01, gamma=0.995, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.01, gamma=0.995, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.01, gamma=0.995, epsilon=0.2 complete.\n",
            "Trial 4 for alpha=0.01, gamma=0.995, epsilon=0.2 complete.\n",
            "Trial 5 for alpha=0.01, gamma=0.995, epsilon=0.2 complete.\n",
            "Trials for alpha=0.01, gamma=0.995, epsilon=0.2 complete. Average reward: 123.8\n",
            "Trial 1 for alpha=0.01, gamma=0.995, epsilon=0.25 complete.\n",
            "Trial 2 for alpha=0.01, gamma=0.995, epsilon=0.25 complete.\n",
            "Trial 3 for alpha=0.01, gamma=0.995, epsilon=0.25 complete.\n",
            "Trial 4 for alpha=0.01, gamma=0.995, epsilon=0.25 complete.\n",
            "Trial 5 for alpha=0.01, gamma=0.995, epsilon=0.25 complete.\n",
            "Trials for alpha=0.01, gamma=0.995, epsilon=0.25 complete. Average reward: 176.8\n",
            "Trial 1 for alpha=0.01, gamma=0.999, epsilon=0.15 complete.\n",
            "Trial 2 for alpha=0.01, gamma=0.999, epsilon=0.15 complete.\n",
            "Trial 3 for alpha=0.01, gamma=0.999, epsilon=0.15 complete.\n",
            "Trial 4 for alpha=0.01, gamma=0.999, epsilon=0.15 complete.\n",
            "Trial 5 for alpha=0.01, gamma=0.999, epsilon=0.15 complete.\n",
            "Trials for alpha=0.01, gamma=0.999, epsilon=0.15 complete. Average reward: 132.4\n",
            "Trial 1 for alpha=0.01, gamma=0.999, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.01, gamma=0.999, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.01, gamma=0.999, epsilon=0.2 complete.\n",
            "Trial 4 for alpha=0.01, gamma=0.999, epsilon=0.2 complete.\n",
            "Trial 5 for alpha=0.01, gamma=0.999, epsilon=0.2 complete.\n",
            "Trials for alpha=0.01, gamma=0.999, epsilon=0.2 complete. Average reward: 246.4\n",
            "Trial 1 for alpha=0.01, gamma=0.999, epsilon=0.25 complete.\n",
            "Trial 2 for alpha=0.01, gamma=0.999, epsilon=0.25 complete.\n",
            "Trial 3 for alpha=0.01, gamma=0.999, epsilon=0.25 complete.\n",
            "Trial 4 for alpha=0.01, gamma=0.999, epsilon=0.25 complete.\n",
            "Trial 5 for alpha=0.01, gamma=0.999, epsilon=0.25 complete.\n",
            "Trials for alpha=0.01, gamma=0.999, epsilon=0.25 complete. Average reward: 137.0\n",
            "Trial 1 for alpha=0.05, gamma=0.99, epsilon=0.15 complete.\n",
            "Trial 2 for alpha=0.05, gamma=0.99, epsilon=0.15 complete.\n",
            "Trial 3 for alpha=0.05, gamma=0.99, epsilon=0.15 complete.\n",
            "Trial 4 for alpha=0.05, gamma=0.99, epsilon=0.15 complete.\n",
            "Trial 5 for alpha=0.05, gamma=0.99, epsilon=0.15 complete.\n",
            "Trials for alpha=0.05, gamma=0.99, epsilon=0.15 complete. Average reward: 352.2\n",
            "Trial 1 for alpha=0.05, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.05, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.05, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 4 for alpha=0.05, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 5 for alpha=0.05, gamma=0.99, epsilon=0.2 complete.\n",
            "Trials for alpha=0.05, gamma=0.99, epsilon=0.2 complete. Average reward: 153.2\n",
            "Trial 1 for alpha=0.05, gamma=0.99, epsilon=0.25 complete.\n",
            "Trial 2 for alpha=0.05, gamma=0.99, epsilon=0.25 complete.\n",
            "Trial 3 for alpha=0.05, gamma=0.99, epsilon=0.25 complete.\n",
            "Trial 4 for alpha=0.05, gamma=0.99, epsilon=0.25 complete.\n",
            "Trial 5 for alpha=0.05, gamma=0.99, epsilon=0.25 complete.\n",
            "Trials for alpha=0.05, gamma=0.99, epsilon=0.25 complete. Average reward: 315.2\n",
            "Trial 1 for alpha=0.05, gamma=0.995, epsilon=0.15 complete.\n",
            "Trial 2 for alpha=0.05, gamma=0.995, epsilon=0.15 complete.\n",
            "Trial 3 for alpha=0.05, gamma=0.995, epsilon=0.15 complete.\n",
            "Trial 4 for alpha=0.05, gamma=0.995, epsilon=0.15 complete.\n",
            "Trial 5 for alpha=0.05, gamma=0.995, epsilon=0.15 complete.\n",
            "Trials for alpha=0.05, gamma=0.995, epsilon=0.15 complete. Average reward: 253.4\n",
            "Trial 1 for alpha=0.05, gamma=0.995, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.05, gamma=0.995, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.05, gamma=0.995, epsilon=0.2 complete.\n",
            "Trial 4 for alpha=0.05, gamma=0.995, epsilon=0.2 complete.\n",
            "Trial 5 for alpha=0.05, gamma=0.995, epsilon=0.2 complete.\n",
            "Trials for alpha=0.05, gamma=0.995, epsilon=0.2 complete. Average reward: 177.8\n",
            "Trial 1 for alpha=0.05, gamma=0.995, epsilon=0.25 complete.\n",
            "Trial 2 for alpha=0.05, gamma=0.995, epsilon=0.25 complete.\n",
            "Trial 3 for alpha=0.05, gamma=0.995, epsilon=0.25 complete.\n",
            "Trial 4 for alpha=0.05, gamma=0.995, epsilon=0.25 complete.\n",
            "Trial 5 for alpha=0.05, gamma=0.995, epsilon=0.25 complete.\n",
            "Trials for alpha=0.05, gamma=0.995, epsilon=0.25 complete. Average reward: 326.0\n",
            "Trial 1 for alpha=0.05, gamma=0.999, epsilon=0.15 complete.\n",
            "Trial 2 for alpha=0.05, gamma=0.999, epsilon=0.15 complete.\n",
            "Trial 3 for alpha=0.05, gamma=0.999, epsilon=0.15 complete.\n",
            "Trial 4 for alpha=0.05, gamma=0.999, epsilon=0.15 complete.\n",
            "Trial 5 for alpha=0.05, gamma=0.999, epsilon=0.15 complete.\n",
            "Trials for alpha=0.05, gamma=0.999, epsilon=0.15 complete. Average reward: 283.2\n",
            "Trial 1 for alpha=0.05, gamma=0.999, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.05, gamma=0.999, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.05, gamma=0.999, epsilon=0.2 complete.\n",
            "Trial 4 for alpha=0.05, gamma=0.999, epsilon=0.2 complete.\n",
            "Trial 5 for alpha=0.05, gamma=0.999, epsilon=0.2 complete.\n",
            "Trials for alpha=0.05, gamma=0.999, epsilon=0.2 complete. Average reward: 163.2\n",
            "Trial 1 for alpha=0.05, gamma=0.999, epsilon=0.25 complete.\n",
            "Trial 2 for alpha=0.05, gamma=0.999, epsilon=0.25 complete.\n",
            "Trial 3 for alpha=0.05, gamma=0.999, epsilon=0.25 complete.\n",
            "Trial 4 for alpha=0.05, gamma=0.999, epsilon=0.25 complete.\n",
            "Trial 5 for alpha=0.05, gamma=0.999, epsilon=0.25 complete.\n",
            "Trials for alpha=0.05, gamma=0.999, epsilon=0.25 complete. Average reward: 158.2\n",
            "Trial 1 for alpha=0.1, gamma=0.99, epsilon=0.15 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.99, epsilon=0.15 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.99, epsilon=0.15 complete.\n",
            "Trial 4 for alpha=0.1, gamma=0.99, epsilon=0.15 complete.\n",
            "Trial 5 for alpha=0.1, gamma=0.99, epsilon=0.15 complete.\n",
            "Trials for alpha=0.1, gamma=0.99, epsilon=0.15 complete. Average reward: 216.4\n",
            "Trial 1 for alpha=0.1, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 4 for alpha=0.1, gamma=0.99, epsilon=0.2 complete.\n",
            "Trial 5 for alpha=0.1, gamma=0.99, epsilon=0.2 complete.\n",
            "Trials for alpha=0.1, gamma=0.99, epsilon=0.2 complete. Average reward: 255.0\n",
            "Trial 1 for alpha=0.1, gamma=0.99, epsilon=0.25 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.99, epsilon=0.25 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.99, epsilon=0.25 complete.\n",
            "Trial 4 for alpha=0.1, gamma=0.99, epsilon=0.25 complete.\n",
            "Trial 5 for alpha=0.1, gamma=0.99, epsilon=0.25 complete.\n",
            "Trials for alpha=0.1, gamma=0.99, epsilon=0.25 complete. Average reward: 189.0\n",
            "Trial 1 for alpha=0.1, gamma=0.995, epsilon=0.15 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.995, epsilon=0.15 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.995, epsilon=0.15 complete.\n",
            "Trial 4 for alpha=0.1, gamma=0.995, epsilon=0.15 complete.\n",
            "Trial 5 for alpha=0.1, gamma=0.995, epsilon=0.15 complete.\n",
            "Trials for alpha=0.1, gamma=0.995, epsilon=0.15 complete. Average reward: 275.2\n",
            "Trial 1 for alpha=0.1, gamma=0.995, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.995, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.995, epsilon=0.2 complete.\n",
            "Trial 4 for alpha=0.1, gamma=0.995, epsilon=0.2 complete.\n",
            "Trial 5 for alpha=0.1, gamma=0.995, epsilon=0.2 complete.\n",
            "Trials for alpha=0.1, gamma=0.995, epsilon=0.2 complete. Average reward: 193.4\n",
            "Trial 1 for alpha=0.1, gamma=0.995, epsilon=0.25 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.995, epsilon=0.25 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.995, epsilon=0.25 complete.\n",
            "Trial 4 for alpha=0.1, gamma=0.995, epsilon=0.25 complete.\n",
            "Trial 5 for alpha=0.1, gamma=0.995, epsilon=0.25 complete.\n",
            "Trials for alpha=0.1, gamma=0.995, epsilon=0.25 complete. Average reward: 188.8\n",
            "Trial 1 for alpha=0.1, gamma=0.999, epsilon=0.15 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.999, epsilon=0.15 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.999, epsilon=0.15 complete.\n",
            "Trial 4 for alpha=0.1, gamma=0.999, epsilon=0.15 complete.\n",
            "Trial 5 for alpha=0.1, gamma=0.999, epsilon=0.15 complete.\n",
            "Trials for alpha=0.1, gamma=0.999, epsilon=0.15 complete. Average reward: 220.6\n",
            "Trial 1 for alpha=0.1, gamma=0.999, epsilon=0.2 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.999, epsilon=0.2 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.999, epsilon=0.2 complete.\n",
            "Trial 4 for alpha=0.1, gamma=0.999, epsilon=0.2 complete.\n",
            "Trial 5 for alpha=0.1, gamma=0.999, epsilon=0.2 complete.\n",
            "Trials for alpha=0.1, gamma=0.999, epsilon=0.2 complete. Average reward: 161.4\n",
            "Trial 1 for alpha=0.1, gamma=0.999, epsilon=0.25 complete.\n",
            "Trial 2 for alpha=0.1, gamma=0.999, epsilon=0.25 complete.\n",
            "Trial 3 for alpha=0.1, gamma=0.999, epsilon=0.25 complete.\n",
            "Trial 4 for alpha=0.1, gamma=0.999, epsilon=0.25 complete.\n",
            "Trial 5 for alpha=0.1, gamma=0.999, epsilon=0.25 complete.\n",
            "Trials for alpha=0.1, gamma=0.999, epsilon=0.25 complete. Average reward: 189.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = max(results, key=results.get)\n",
        "best_reward = results[best_params]\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Best Average Reward: {best_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01Yco38FE5b7",
        "outputId": "2c78462b-2876-4145-b43b-a7cef86d28d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: (0.05, 0.99, 0.15)\n",
            "Best Average Reward: 352.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best hyperparameters are as follows:\n",
        "\n",
        "Alpha = 0.05\n",
        "\n",
        "Gamma = 0.99\n",
        "\n",
        "Epsilon = 0.15, 15% exploration\n",
        "\n",
        "Now, train the agent using the best combination of hyperparameters for 10000 episodes - or until best performance."
      ],
      "metadata": {
        "id": "Rd0tmB-JGVoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "agent = MonteCarloAgent(env.action_space.n, alpha=0.05, gamma=0.99, epsilon=0.15)\n",
        "agent.reset_q_table()\n",
        "\n",
        "#train\n",
        "episode_results = []\n",
        "episode_num = 0\n",
        "consistent = False\n",
        "\n",
        "while not consistent:\n",
        "  observation = env.reset()\n",
        "  episode = []\n",
        "  cumulative_reward = 0\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    state = discretize_state(observation)\n",
        "    action = agent.choose_action(state)\n",
        "    observation, reward, done, info = env.step(action)\n",
        "\n",
        "    cart_pos, cart_vel, pole_angle, pole_vel = observation\n",
        "\n",
        "    # termination check based on constraints\n",
        "    if abs(pole_angle) > 0.2094 or abs(cart_pos) > 2.4:\n",
        "        done = True  # terminate episode\n",
        "\n",
        "    episode.append((state, action, reward))\n",
        "    cumulative_reward += reward\n",
        "\n",
        "  agent.update_Q(episode)\n",
        "  episode_num += 1\n",
        "\n",
        "  agent.decay_epsilon()\n",
        "  episode_results.append(cumulative_reward)\n",
        "\n",
        "  # if results = 500 for last 100 episodes, stop training\n",
        "  if episode_num >= 100:\n",
        "    score = episode_results[-100:]\n",
        "    if (episode_num+1) % 100 == 0:\n",
        "      print(f\"Episode {episode_num + 1}: Score = {np.mean(score)}\") # score is the avg rewards of last 100 episodes\n",
        "    if np.mean(score) == 500 or episode_num == 10000:\n",
        "      consistent = True\n",
        "      print(\"Training stopped due to consistent performance.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOutmavkGmev",
        "outputId": "bf2bf623-b46e-49f4-9a84-aea314c8c325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 200: Score = 193.61\n",
            "Episode 300: Score = 187.95\n",
            "Episode 400: Score = 220.52\n",
            "Episode 500: Score = 181.64\n",
            "Episode 600: Score = 267.38\n",
            "Episode 700: Score = 212.33\n",
            "Episode 800: Score = 227.67\n",
            "Episode 900: Score = 214.31\n",
            "Episode 1000: Score = 224.33\n",
            "Episode 1100: Score = 239.74\n",
            "Episode 1200: Score = 176.47\n",
            "Episode 1300: Score = 403.71\n",
            "Episode 1400: Score = 360.95\n",
            "Episode 1500: Score = 369.05\n",
            "Episode 1600: Score = 359.03\n",
            "Episode 1700: Score = 267.95\n",
            "Episode 1800: Score = 342.61\n",
            "Episode 1900: Score = 341.37\n",
            "Episode 2000: Score = 285.46\n",
            "Episode 2100: Score = 334.94\n",
            "Episode 2200: Score = 355.79\n",
            "Episode 2300: Score = 364.66\n",
            "Episode 2400: Score = 273.97\n",
            "Episode 2500: Score = 371.96\n",
            "Episode 2600: Score = 370.71\n",
            "Episode 2700: Score = 276.23\n",
            "Episode 2800: Score = 358.17\n",
            "Episode 2900: Score = 322.76\n",
            "Episode 3000: Score = 436.1\n",
            "Episode 3100: Score = 362.97\n",
            "Episode 3200: Score = 415.0\n",
            "Episode 3300: Score = 291.31\n",
            "Episode 3400: Score = 282.46\n",
            "Episode 3500: Score = 322.8\n",
            "Episode 3600: Score = 295.19\n",
            "Episode 3700: Score = 165.45\n",
            "Episode 3800: Score = 295.63\n",
            "Episode 3900: Score = 384.9\n",
            "Episode 4000: Score = 306.74\n",
            "Episode 4100: Score = 372.0\n",
            "Episode 4200: Score = 305.69\n",
            "Episode 4300: Score = 265.86\n",
            "Episode 4400: Score = 338.16\n",
            "Episode 4500: Score = 372.9\n",
            "Episode 4600: Score = 228.19\n",
            "Episode 4700: Score = 260.45\n",
            "Episode 4800: Score = 348.29\n",
            "Episode 4900: Score = 318.77\n",
            "Episode 5000: Score = 309.51\n",
            "Episode 5100: Score = 285.93\n",
            "Episode 5200: Score = 397.31\n",
            "Episode 5300: Score = 371.96\n",
            "Episode 5400: Score = 399.32\n",
            "Episode 5500: Score = 410.02\n",
            "Episode 5600: Score = 320.8\n",
            "Episode 5700: Score = 291.35\n",
            "Episode 5800: Score = 250.47\n",
            "Episode 5900: Score = 315.14\n",
            "Episode 6000: Score = 372.9\n",
            "Episode 6100: Score = 372.88\n",
            "Episode 6200: Score = 404.37\n",
            "Episode 6300: Score = 398.36\n",
            "Episode 6400: Score = 188.64\n",
            "Episode 6500: Score = 348.0\n",
            "Episode 6600: Score = 360.15\n",
            "Episode 6700: Score = 368.17\n",
            "Episode 6800: Score = 309.58\n",
            "Episode 6900: Score = 269.08\n",
            "Episode 7000: Score = 112.69\n",
            "Episode 7100: Score = 273.23\n",
            "Episode 7200: Score = 283.59\n",
            "Episode 7300: Score = 272.97\n",
            "Episode 7400: Score = 322.48\n",
            "Episode 7500: Score = 375.39\n",
            "Episode 7600: Score = 397.92\n",
            "Episode 7700: Score = 259.83\n",
            "Episode 7800: Score = 340.12\n",
            "Episode 7900: Score = 222.71\n",
            "Episode 8000: Score = 307.09\n",
            "Episode 8100: Score = 274.78\n",
            "Episode 8200: Score = 346.95\n",
            "Episode 8300: Score = 259.81\n",
            "Episode 8400: Score = 230.56\n",
            "Episode 8500: Score = 296.69\n",
            "Episode 8600: Score = 296.29\n",
            "Episode 8700: Score = 298.16\n",
            "Episode 8800: Score = 312.95\n",
            "Episode 8900: Score = 375.4\n",
            "Episode 9000: Score = 313.77\n",
            "Episode 9100: Score = 313.45\n",
            "Episode 9200: Score = 290.36\n",
            "Episode 9300: Score = 302.42\n",
            "Episode 9400: Score = 314.19\n",
            "Episode 9500: Score = 288.7\n",
            "Episode 9600: Score = 329.19\n",
            "Episode 9700: Score = 298.9\n",
            "Episode 9800: Score = 322.26\n",
            "Episode 9900: Score = 332.14\n",
            "Episode 10000: Score = 330.2\n",
            "Training stopped due to consistent performance.\n"
          ]
        }
      ]
    }
  ]
}